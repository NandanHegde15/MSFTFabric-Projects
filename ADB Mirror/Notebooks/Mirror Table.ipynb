{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe9c9596-27a2-4393-a318-61fd4c9b266e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tenant_id = dbutils.secrets.get(scope=\"KeyVault\", key=\"tenentId\")\n",
    "client_id = dbutils.secrets.get(scope=\"KeyVault\", key=\"ADB-Mirror-SPN-ClientId\")\n",
    "client_secret = dbutils.secrets.get(scope=\"KeyVault\", key=\"ADB-Mirror-SPN-Secret\")\n",
    "WHID = dbutils.secrets.get(scope=\"KeyVault\", key=\"ADB-Mirror-WHID\")\n",
    "WSID = dbutils.secrets.get(scope=\"KeyVault\", key=\"ADB-Mirror-WSID\")\n",
    "\n",
    "base_url = f\"abfss://{WHID}@onelake.dfs.fabric.microsoft.com/{WSID}/Files/LandingZone\"\n",
    "primaryKeys = {\"test_catalog.sales_data.orders\": \"order_id\"\n",
    "            #    ,\"test_catalog.sales_data.customers\": \"customer_id\"\n",
    "               }\n",
    "\n",
    "\n",
    "spark.conf.set(\"fs.azure.account.auth.type.onelake.dfs.fabric.microsoft.com\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.onelake.dfs.fabric.microsoft.com\",\n",
    "                \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.onelake.dfs.fabric.microsoft.com\", client_id)\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.onelake.dfs.fabric.microsoft.com\", client_secret)\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.onelake.dfs.fabric.microsoft.com\",\n",
    "                f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef9f4655-92ba-4191-8844-f9bc9510064e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "target_urls = {}\n",
    "\n",
    "for full_table_path, pk in primaryKeys.items():\n",
    "    parts = full_table_path.split(\".\")\n",
    "    if len(parts) != 3:\n",
    "        raise ValueError(f\"Invalid table path format: {full_table_path}\")\n",
    "\n",
    "    catalog, schema, table = parts\n",
    "    target_url = f\"{base_url}/{schema}.schema/{table}\"\n",
    "    target_urls[full_table_path] = target_url\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7de28daa-4fdc-4edc-bf5b-efcb1b62763f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from py4j.protocol import Py4JJavaError\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import lit, when, col\n",
    "from datetime import datetime\n",
    "\n",
    "def get_last_processed_version(table_name: str):\n",
    "    \"\"\"Returns last processed version if exists, else None.\"\"\"\n",
    "    try:\n",
    "        df = spark.table(\"test_catalog.config.cdc_control\") \\\n",
    "                  .filter(F.col(\"table_name\") == table_name)\n",
    "        if df.count() == 0:\n",
    "            return False  # no record → initial load\n",
    "        else:\n",
    "            return df.select(\"last_processed_version\").collect()[0][0]\n",
    "    except Exception:\n",
    "        # control table itself might not exist on first run\n",
    "        return False\n",
    "\n",
    "\n",
    "def write_metadata_file_safe(target_dir: str, table_name: str):\n",
    "    key_col = primaryKeys.get(table_name)\n",
    "    metadata = {\"KeyColumns\": [key_col]} if key_col else {\"KeyColumns\": []}\n",
    "    metadata_json = json.dumps(metadata, indent=4)\n",
    "\n",
    "    dbutils.fs.put(f\"{target_dir}/_metadata.json\", metadata_json, overwrite=True)\n",
    "    print(f\"Metadata file written to {target_dir}/_metadata.json\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a505204d-84d8-43b2-b65a-cc189264638a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def upsert_cdc_log(table_name: str, last_processed_version: int, last_processed_timestamp):\n",
    "    \"\"\"Insert or update CDC control table with latest processed version and timestamp.\"\"\"\n",
    "    last_processed_timestamp_str = str(last_processed_timestamp)\n",
    "    spark.sql(f\"\"\"\n",
    "        MERGE INTO test_catalog.config.cdc_control AS target\n",
    "        USING (SELECT\n",
    "                  '{table_name}' AS table_name,\n",
    "                  {last_processed_version} AS last_processed_version,\n",
    "                  TIMESTAMP('{last_processed_timestamp_str}') AS last_processed_timestamp,\n",
    "                  current_timestamp() AS updated_at\n",
    "               ) AS source\n",
    "        ON target.table_name = source.table_name\n",
    "        WHEN MATCHED THEN\n",
    "          UPDATE SET\n",
    "            target.last_processed_version = source.last_processed_version,\n",
    "            target.last_processed_timestamp = source.last_processed_timestamp,\n",
    "            target.updated_at = source.updated_at\n",
    "        WHEN NOT MATCHED THEN\n",
    "          INSERT (table_name, last_processed_version, last_processed_timestamp, updated_at)\n",
    "          VALUES (source.table_name, source.last_processed_version, source.last_processed_timestamp, source.updated_at)\n",
    "    \"\"\")\n",
    "    print(f\"CDC control updated → {table_name}: version={last_processed_version}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92d57b20-3644-4f67-b520-beafe176668f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def full_initial_load_single_parquet(target_dir: str, table_name: str):\n",
    "\n",
    "    write_metadata_file_safe(target_dir, table_name)\n",
    "\n",
    "    df = spark.table(table_name)\n",
    "    local_tmp_dir = \"dbfs:/tmp/local_parquet\"\n",
    "    dbutils.fs.rm(local_tmp_dir, recurse=True)\n",
    "\n",
    "    df.coalesce(1).write.mode(\"overwrite\").parquet(local_tmp_dir)\n",
    "\n",
    "    files = [f.path for f in dbutils.fs.ls(local_tmp_dir) if f.name.endswith(\".parquet\")]\n",
    "    if not files:\n",
    "        raise Exception(\"No parquet file generated in temp_dir\")\n",
    "\n",
    "    temp_file = files[0]\n",
    "    target_file = f\"{target_dir}/00000000000000000001.parquet\"\n",
    "\n",
    "    # Copy only the parquet file (skip _SUCCESS, logs)\n",
    "    dbutils.fs.cp(temp_file, target_file)\n",
    "    dbutils.fs.rm(local_tmp_dir, recurse=True)\n",
    "\n",
    "    # Get latest version & timestamp and update control log\n",
    "    latest = spark.sql(f\"DESCRIBE HISTORY {table_name}\").orderBy(F.desc(\"version\")).first()\n",
    "    upsert_cdc_log(table_name, latest['version'], latest['timestamp'])\n",
    "\n",
    "    print(f\"Full load complete: {target_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9dd6b08-8672-41b9-a0f4-4f23531596df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_next_parquet_number(target_dir: str) -> str:\n",
    "    try:\n",
    "        files = [f.name for f in dbutils.fs.ls(target_dir) if f.name.endswith(\".parquet\")]\n",
    "        if not files:\n",
    "            return\n",
    "\n",
    "        numbers = []\n",
    "        for f in files:\n",
    "            match = re.match(r\"(\\d+)\\.parquet\", f)\n",
    "            if match:\n",
    "                numbers.append(int(match.group(1)))\n",
    "\n",
    "        if not numbers:\n",
    "            next_num = 1\n",
    "        else:\n",
    "            next_num = max(numbers) + 1\n",
    "        return f\"{next_num:020d}.parquet\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error while finding next parquet number for {target_dir}: {e}\")\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00a7c651-730b-4510-9c17-5817301a1c0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, lit, when\n",
    "\n",
    "def run_delta_load(\n",
    "    table_name: str,\n",
    "    target_dir: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs delta load for the given source table using Unity Catalog CDC (table_changes).\n",
    "    Writes delta data as parquet (single file) with __rowMarker__ column.\n",
    "    Updates CDC log table via upsert_cdc_log().\n",
    "    \"\"\"\n",
    "    control_table = \"test_catalog.config.cdc_control\"\n",
    "    try:\n",
    "        ctrl_df = spark.read.table(control_table).filter(col(\"table_name\") == table_name)\n",
    "        if ctrl_df.count() == 0:\n",
    "            print(f\"No control entry found for {table_name}. Please run initial load first.\")\n",
    "            return\n",
    "\n",
    "        last_version = ctrl_df.select(\"last_processed_version\").collect()[0][0]\n",
    "        print(f\"Last processed version for {table_name}: {last_version}\")\n",
    "\n",
    "        # --- Get current latest version of the table ---\n",
    "        latest = spark.sql(f\"DESCRIBE HISTORY {table_name}\").orderBy(F.desc(\"version\")).first()\n",
    "        new_version = latest['version']\n",
    "        new_timestamp = latest['timestamp']\n",
    "\n",
    "        # --- Validate versions before running CDC ---\n",
    "        if last_version >= new_version:\n",
    "            print(f\"No new changes. Latest version ({new_version}) <= last processed ({last_version}).\")\n",
    "            return\n",
    "\n",
    "        # --- FIX: table_changes is inclusive → start from last_version + 1 ---\n",
    "        start_version = int(last_version) + 1\n",
    "        end_version = int(new_version)\n",
    "\n",
    "        query = f\"SELECT * FROM table_changes('{table_name}', {start_version}, {end_version})\"\n",
    "        changes_df = spark.sql(query)\n",
    "\n",
    "        # --- FIX: Unity Catalog doesn't support .rdd, so use .limit(1).count() ---\n",
    "        if changes_df.limit(1).count() == 0:\n",
    "            print(\"No new changes found.\")\n",
    "            return\n",
    "\n",
    "        changes_df = (\n",
    "            changes_df\n",
    "            .withColumn(\n",
    "                \"__rowMarker__\",\n",
    "                when(col(\"_change_type\") == \"insert\", lit(0))\n",
    "                .when(col(\"_change_type\") == \"update_postimage\", lit(1))\n",
    "                .when(col(\"_change_type\") == \"delete\", lit(2))\n",
    "                .otherwise(lit(None))\n",
    "            )\n",
    "            .drop(\"_change_type\", \"_commit_version\", \"_commit_timestamp\")\n",
    "        )\n",
    "        changes_df = changes_df.filter(changes_df[\"__rowMarker__\"].isNotNull()).orderBy(col(\"__rowMarker__\"))\n",
    "\n",
    "        local_tmp_dir = \"dbfs:/tmp/local_parquet\"\n",
    "        dbutils.fs.rm(local_tmp_dir, recurse=True)\n",
    "\n",
    "        changes_df.coalesce(1).write.mode(\"overwrite\").parquet(local_tmp_dir)\n",
    "\n",
    "        files = [f.path for f in dbutils.fs.ls(local_tmp_dir) if f.name.endswith(\".parquet\")]\n",
    "        if not files:\n",
    "            raise Exception(\"No parquet file generated in temp_dir\")\n",
    "\n",
    "        temp_file = files[0]\n",
    "        next_file = get_next_parquet_number(target_dir)\n",
    "        target_file = f\"{target_dir}/{next_file}\"\n",
    "\n",
    "        dbutils.fs.cp(temp_file, target_file)\n",
    "        dbutils.fs.rm(local_tmp_dir, recurse=True)\n",
    "\n",
    "        print(f\"Delta parquet file created: {target_file}\")\n",
    "\n",
    "        upsert_cdc_log(table_name, new_version, new_timestamp)\n",
    "        print(f\"Delta load completed for {table_name}. Updated version → {new_version}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Delta load failed for {table_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "183974e3-d7e4-4db2-9ca3-33fdfea7a451",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for full_table_name, pk in primaryKeys.items():\n",
    "    if not get_last_processed_version(full_table_name):\n",
    "        full_initial_load_single_parquet(target_urls[full_table_name], full_table_name)\n",
    "    else:\n",
    "        print(f\"Skipping full load for {full_table_name} as it has already been processed\")\n",
    "        run_delta_load(full_table_name, target_urls[full_table_name])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Mirror Table",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}